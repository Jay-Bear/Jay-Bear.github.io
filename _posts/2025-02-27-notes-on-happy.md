# Notes on Haskell's Happy LALR(1) Parser

The main intention for this is to help students understand a few potentially
confusing things about LALR(1) parsing, in particular the one used by Happy
(which is basically Haskell's YACC). This serves as more of a collection of
notes – *mostly those I've written in Discord messages* – but it can be
anything else.

### Foreword on Terminology

Take the example
```
Exp :: {Expression}
  : Exp '+' Exp {Add $1 $3}
  | Exp '*' Exp {Mul $1 $3}
```
I'm going to use the term **production** to refer to a non-terminal block
(`Exp`) as a whole, and **expansion** to refer to each individual element of
the non-terminal block.

&nbsp;

## Braces in Production Rules are just Haskell

For anyone who might be confused about how the braces operate in Happy
production rules, for example,\
`let var '=' Exp in Exp {Let $2 $4 $6}`,\
just note that for both Alex and Happy, pretty much anything in the braces is
Haskell and is almost entirely inserted into the generated files verbatim. *Of
course* you may then argue that `$2` is not valid Haskell – *and you are
correct*, I'm hiding a lot of extra technical detail here, but this `$2` is
most likely going to be converted to `happy_var_2` in the generated file, which
is an argument passed in place of the `$2`:\
`Let happy_var_2 happy_var_4 happy_var_6`

This `$n` format just tells Happy to replace whatever is in that position with
whatever the `n`th term produces as part of a reduction. For example, with a
simple grammar that encodes Peano naturals:
```
Nat :: {Int}
  : Succ Nat {1 + $2}
  | Zero     {0     }
```
the `Zero` expansion produces literally a `0 :: Int` in the Happy-generated
function. The `Succ Nat` expansion takes the value produced by reducing `Nat`
and adds `1` to it, because `Nat` appears in the 2nd position (`$2`). When the
token sequence `Succ Succ Nat` is parsed, this performs equivalent to
`1 + 1 + 0`, which produces `2`.

If you look in the generated file, you'll find the term encased
in some other generated functions (most likely called `happyReduction_n` where
`n` is some integer). For me, that generates:
```haskell
happyReduction_2 _ =
  HappyAbsSyn4 (0)
```
where `HappyAbsSyn4 :: HappyAbsSyn` is a generated data constructor. In most
cases where we use such a parser, we wish to generate an abstract syntax tree,
and often this is done by defining custom data types. In the case of the `Let`
statement at the beginning of this section, that `Let` is a data constructor
for a certain type which is defined in a separate bit of Haskell (usually in
the postamble or in a separate imported file).

It's also important to note that **each production must produce values of the
same type**. If you look at the `Nat` example, I specify that the production
will give `Int` values, but this type declaration is optional.

### Getting Values from Tokens

When you state your tokens in Happy, you may use `$$` for certain values
contained in the token type, such as a string or integer. This is so Happy can
use these values when you do `$n` later on. This works equivalently to
pattern matching – for example you may be familiar with
```haskell
fromJust :: Maybe a
         -> a
-- Error case for `Nothing` here.
fromJust (Just x) =
  x
```
which you can view as *'extracting'* the value contained in the value of type
`Maybe a`. This is how the `$$` works in your token statements. If I have a
token data constructor `TokenVar :: String -> Token` from the data type
definition
```haskell
data Token =
  ...
  | TokenVar String
  ...
```
then the token statement `var {TokenVar $$}` performs this pattern matching by
producing a reduction that converts `var` to the string contained in the
token.

&nbsp;

## Shifting and Reducing: The Fundamentals

Depending on when you're reading this, you may already be familiar with the
terms **shift** and **reduce** in the context of parsing, specifically with
Happy. You may have also encountered a shift/reduce or reduce/reduce conflict.
It's important to know what these terms mean and the context behind the
conflicts.

### LALR Parsing and Pushdown Automata

*Look-ahead, left-to-right, rightmost derivation* (LALR) parsers are a
simplification of *left-to-right, rightmost derivation* (LR) parsers. These are
bottom-up parsers. Usually these parsers are followed by a number which states
their *look-ahead*, which is the number of tokens the parser can literally
"look ahead" in order to parse the sequence. In most cases this is LALR(1) and
LR(1). *Confusingly* both of these use look-ahead, but the primary difference
is how this look-ahead is used and certain simplifications. The reality is that
an LALR(1) parser can parse all LR(0) languages, but not all LR(1) languages.

Feel free to read more about these types of parsers and their differences, but
that's outside the scope of this post.

The important thing to note is that LALR(1) parsers – generated by Happy –
can be described with pushdown automata (PDA). For those unfamiliar or need
their memory jogged, I'm gonna use the formal definition Wikipedia defines
(since *everyone* seems to have their own way) which says a PDA is a 7-tuple

$$M=\left(Q,\,\Sigma,\,\Gamma,\,\delta,\,q_0,\,Z,\,F\right)$$

where
- $Q$ is a finite set of states.
- $\Sigma$ is a finite set called the input alphabet.
- $\Gamma$ is a finite set called the stack alphabet.
- $\delta\subseteq Q\times\left(\Sigma\cup\left\{\varepsilon\right\}\right)
  \times\Gamma\times Q\times\Gamma^*$ is a finite set called the transition
  relation.
- $q_0\in Q$ is the start state.
- $Z\in\Gamma$ is the initial stack symbol.
- $F\subseteq Q$ is the set of accepting states.

Specifically, an element of $\delta$ describes a transition
$\left(q_n,\,\sigma_a,\,\gamma_u\right)\mapsto\left(q_m,\,\gamma_{v_1}
\gamma_{v_2}\cdots\gamma_{v_t}\right)$ which
takes a current state, a current input, and current topmost stack symbol, and
produces a new state and a string to push to the stack *after* popping.

Deterministic PDAs can recognize all deterministic context-free languages,
which makes them ideal for parsers. The stack (where the term *pushdown* comes
from) is the biggest reason for this. Additionally, the stack allows the parser
to construct an abstract syntax tree (AST) directly on the stack itself.

### Shifting

In a LALR(1) parser, one of the primary actions is called **shifting**, which
is simply the action of moving the current token onto the stack. This is often
followed by a transition of state. Shifts can be described as transitions of
the form

$$\left(q_n,\,\sigma_a,\,\gamma_u\right)\mapsto\left(q_m,\,\tau\!\left(
\sigma_a\right)\right)$$

where $\tau:\Sigma\to\Gamma$ *encodes* the token as part of the stack alphabet.
This encoding is important for constructing the AST and as part of the
**reduction** operation.

### Reducing

The second primary action is called **reducing**, which takes pops items from
the stack, modifies them, and pushes to the stack.
